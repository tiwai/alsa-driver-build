--- ../alsa-kernel/core/memalloc.c	2014-02-14 08:10:32.318887692 +0100
+++ memalloc.c	2014-02-14 10:46:42.307855817 +0100
@@ -1,3 +1,4 @@
+#include "memalloc.inc"
 /*
  *  Copyright (c) by Jaroslav Kysela <perex@perex.cz>
  *                   Takashi Iwai <tiwai@suse.de>
@@ -28,11 +29,131 @@
 #include <sound/memalloc.h>
 
 /*
+ *  Hacks
+ */
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 5)
+static void *snd_dma_alloc_coherent1(struct device *dev, size_t size,
+				     dma_addr_t *dma_handle, int flags)
+{
+	if (dev)
+		return dma_alloc_coherent(dev, size, dma_handle, flags);
+	else /* FIXME: dma_alloc_coherent does't always accept dev=NULL */
+		return pci_alloc_consistent(NULL, size, dma_handle);
+}
+
+static void snd_dma_free_coherent1(struct device *dev, size_t size, void *dma_addr,
+				   dma_addr_t dma_handle)
+{
+	if (dev)
+		return dma_free_coherent(dev, size, dma_addr, dma_handle);
+	else
+		return pci_free_consistent(NULL, size, dma_addr, dma_handle);
+}
+
+#undef dma_alloc_coherent
+#define dma_alloc_coherent snd_dma_alloc_coherent1
+#undef dma_free_coherent
+#define dma_free_coherent snd_dma_free_coherent1
+#endif /* < 2.6.5 */
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 26)
+#if defined(__i386__) || defined(__ppc__) || defined(__x86_64__)
+
+/*
+ * A hack to allocate large buffers via dma_alloc_coherent()
+ *
+ * since dma_alloc_coherent always tries GFP_DMA when the requested
+ * pci memory region is below 32bit, it happens quite often that even
+ * 2 order of pages cannot be allocated.
+ *
+ * so in the following, we allocate at first without dma_mask, so that
+ * allocation will be done without GFP_DMA.  if the area doesn't match
+ * with the requested region, then realloate with the original dma_mask
+ * again.
+ *
+ * Really, we want to move this type of thing into dma_alloc_coherent()
+ * so dma_mask doesn't have to be messed with.
+ */
+
+#define VALID_DMA_MASK(dev)	(dev)->dma_mask
+#define GET_DMA_MASK(dev)	*(dev)->dma_mask
+#define SET_DMA_MASK(dev,val)	(*(dev)->dma_mask = (val))
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 5)
+#define GET_COHERENT_DMA_MASK(dev)	(dev)->coherent_dma_mask
+#define SET_COHERENT_DMA_MASK(dev,val)	((dev)->coherent_dma_mask = (val))
+#else
+#define GET_COHERENT_DMA_MASK(dev)	0 /* dummy */
+#define SET_COHERENT_DMA_MASK(dev,val)
+#endif
+
+static void *snd_dma_hack_alloc_coherent(struct device *dev, size_t size,
+					 dma_addr_t *dma_handle,
+					 gfp_t flags)
+{
+	void *ret;
+	u64 dma_mask, coherent_dma_mask;
+
+	if (dev == NULL || !VALID_DMA_MASK(dev))
+		return dma_alloc_coherent(dev, size, dma_handle, flags);
+	dma_mask = GET_DMA_MASK(dev);
+	coherent_dma_mask = GET_COHERENT_DMA_MASK(dev);
+	SET_DMA_MASK(dev, 0xffffffff); 	/* do without masking */
+	SET_COHERENT_DMA_MASK(dev, 0xffffffff); 	/* do without masking */
+	ret = dma_alloc_coherent(dev, size, dma_handle, flags);
+	SET_DMA_MASK(dev, dma_mask);	/* restore */
+	SET_COHERENT_DMA_MASK(dev, coherent_dma_mask)	/* restore */;
+	if (ret) {
+		/* obtained address is out of range? */
+		if (((unsigned long)*dma_handle + size - 1) & ~dma_mask) {
+			/* reallocate with the proper mask */
+			dma_free_coherent(dev, size, ret, *dma_handle);
+			ret = dma_alloc_coherent(dev, size, dma_handle, flags);
+		}
+	} else {
+		/* wish to success now with the proper mask... */
+		if (dma_mask != 0xffffffffUL) {
+			/* allocation with GFP_ATOMIC to avoid the long stall */
+			flags &= ~GFP_KERNEL;
+			flags |= GFP_ATOMIC;
+			ret = dma_alloc_coherent(dev, size, dma_handle, flags);
+		}
+	}
+	return ret;
+}
+
+/* redefine dma_alloc_coherent for some architectures */
+#undef dma_alloc_coherent
+#define dma_alloc_coherent snd_dma_hack_alloc_coherent
+
+#endif /* arch */
+#endif /* < 2.6.26 */
+
+
+/*
  *
  *  Generic memory allocators
  *
  */
 
+static void mark_pages(struct page *page, int order)
+{
+#if ! defined(__arm__) && LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 16)
+	struct page *last_page = page + (1 << order);
+	while (page < last_page)
+		SetPageReserved(page++);
+#endif
+}
+
+static void unmark_pages(struct page *page, int order)
+{
+#if ! defined(__arm__) && LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 16)
+	struct page *last_page = page + (1 << order);
+	while (page < last_page)
+		ClearPageReserved(page++);
+#endif
+}
+
 /**
  * snd_malloc_pages - allocate pages with the given size
  * @size: the size to allocate in bytes
@@ -45,6 +166,7 @@
 void *snd_malloc_pages(size_t size, gfp_t gfp_flags)
 {
 	int pg;
+	void *res;
 
 	if (WARN_ON(!size))
 		return NULL;
@@ -52,7 +174,9 @@
 		return NULL;
 	gfp_flags |= __GFP_COMP;	/* compound page lets parts be mapped */
 	pg = get_order(size);
-	return (void *) __get_free_pages(gfp_flags, pg);
+	if ((res = (void *) __get_free_pages(gfp_flags, pg)) != NULL)
+		mark_pages(virt_to_page(res), pg);
+	return res;
 }
 
 /**
@@ -69,6 +193,7 @@
 	if (ptr == NULL)
 		return;
 	pg = get_order(size);
+	unmark_pages(virt_to_page(ptr), pg);
 	free_pages((unsigned long) ptr, pg);
 }
 
@@ -82,6 +207,7 @@
 /* allocate the coherent DMA pages */
 static void *snd_malloc_dev_pages(struct device *dev, size_t size, dma_addr_t *dma)
 {
+	void *res;
 	int pg;
 	gfp_t gfp_flags;
 
@@ -92,7 +218,11 @@
 		| __GFP_COMP	/* compound page lets parts be mapped */
 		| __GFP_NORETRY /* don't trigger OOM-killer */
 		| __GFP_NOWARN; /* no stack trace print - this call is non-critical */
-	return dma_alloc_coherent(dev, PAGE_SIZE << pg, dma, gfp_flags);
+	res = dma_alloc_coherent(dev, PAGE_SIZE << pg, dma, gfp_flags);
+	if (res != NULL)
+		mark_pages(virt_to_page(res), pg);
+
+	return res;
 }
 
 /* free the coherent DMA pages */
@@ -104,6 +234,7 @@
 	if (ptr == NULL)
 		return;
 	pg = get_order(size);
+	unmark_pages(virt_to_page(ptr), pg);
 	dma_free_coherent(dev, PAGE_SIZE << pg, ptr, dma);
 }
 
@@ -123,6 +254,7 @@
 	dmab->area = NULL;
 	dmab->addr = 0;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 13, 0)
 	if (dev->of_node)
 		pool = of_get_named_gen_pool(dev->of_node, "iram", 0);
 
@@ -133,6 +265,7 @@
 	dmab->private_data = pool;
 
 	dmab->area = gen_pool_dma_alloc(pool, size, &dmab->addr);
+#endif
 }
 
 /**
@@ -141,10 +274,12 @@
  */
 static void snd_free_dev_iram(struct snd_dma_buffer *dmab)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 13, 0)
 	struct gen_pool *pool = dmab->private_data;
 
 	if (pool && dmab->area)
 		gen_pool_free(pool, (unsigned long)dmab->area, dmab->bytes);
+#endif
 }
 #endif /* CONFIG_GENERIC_ALLOCATOR */
 #endif /* CONFIG_HAS_DMA */
@@ -297,3 +432,5 @@
 
 EXPORT_SYMBOL(snd_malloc_pages);
 EXPORT_SYMBOL(snd_free_pages);
+
+#include "memalloc.inc1"
